{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":673416,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":510329,"modelId":524999},{"sourceId":674297,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":511088,"modelId":525774},{"sourceId":674515,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":511280,"modelId":525959},{"sourceId":675427,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":512035,"modelId":526683},{"sourceId":675466,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":512071,"modelId":526721}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install torchvista\n# !pip install requirements.txt\n# !pip install --upgrade datasets\n\nimport datasets\nfrom datasets import load_dataset\nfrom datasets import Dataset\nimport pandas as pd\nimport cv2\nimport numpy as np\nfrom torchvision import transforms as tr\nimport torch\nfrom torch.utils.data import WeightedRandomSampler, DataLoader, random_split\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\n# import torchvista\n# from torchvista import trace_model\nimport sklearn\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport torchvision\n\n\nENSEMBLE_PATH=\"/kaggle/input/ensemble-fai/pytorch/default/1/ensemble.pth\"\nRESCNN_PATH=\"/kaggle/input/rescnn/pytorch/default/1/rescnn_state.pth\"\nCNN_PATH=\"/kaggle/input/cnn/pytorch/default/1/cnn.pth\"\nVIT_PATH=\"/kaggle/input/vit-best/pytorch/default/1/vit_best.pth\"\n \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.is_available():\n    print(torch.cuda.get_device_name(0))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:43:22.349468Z","iopub.execute_input":"2025-12-07T14:43:22.350564Z","iopub.status.idle":"2025-12-07T14:43:22.358328Z","shell.execute_reply.started":"2025-12-07T14:43:22.350526Z","shell.execute_reply":"2025-12-07T14:43:22.357059Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"# Model Architecture","metadata":{}},{"cell_type":"markdown","source":"## CNN architecture","metadata":{}},{"cell_type":"code","source":"class Block(nn.Module):\n  \"\"\"\n  Description: Residual Block architecture modified to be without skip connection\n\n  Input: \n  in_channels: int\n  Number of channels present in input\n  out_channels: int\n  Number of output channels\n  stride: 1\n  Step size a filter uses to move across an image\n  \n  Output: Object\n  Initialized instance of Block\n  \n  \"\"\"\n  def __init__(self, in_channels, out_channels,  stride=1):\n        super(Block, self).__init__()\n        self.conv1=nn.Sequential(\n        nn.Conv2d(in_channels= in_channels,\n                  out_channels= out_channels,\n                  kernel_size=3,\n                  stride=stride,\n                  padding=1  ),\n            nn.BatchNorm2d(num_features= out_channels ),\n            nn.ReLU()\n        )\n\n        self.conv2= nn.Sequential(\n            nn.Conv2d(in_channels= out_channels,\n                      out_channels= out_channels,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1  ),\n            nn.BatchNorm2d(num_features=out_channels )\n        )\n\n        self.resize=nn.Sequential()\n\n        if in_channels != out_channels and stride!=1:\n\n          self.resizer= nn.Sequential(\n              nn.Conv2d(in_channels= in_channels,\n                                out_channels= out_channels,\n                                kernel_size=1,\n                                stride=stride,\n                                padding=0\n                ),\n              nn.BatchNorm2d(num_features=out_channels )\n          )\n\n        # self.fc= nn.Linear(in_features=OUT_CHANNELS , out_features= NUM_CLASSES)\n\n  def forward(self,x):\n        x= self.conv1(x)\n        x= self.conv2(x)\n        if self.resize == True:\n          add_ = F.relu(x)+self.resizer(x) #skip connections\n        else:\n          add_=F.relu(x)\n        return add_\n\n\nclass CNN(nn.Module):\n  \"\"\"\n  Description: CNN model\n\n  Input:  \n  num_classes: int\n  Number of classes involved in classification task\n  stride: Array\n  Step size a filter uses for corresponding block to move across an image\n  blocks: Array\n  Input size of each block in CNN\n  \n  Output: Object\n  Initialized instance of CNN with Residual blocks\n  \n  \"\"\"\n  def __init__(self,blocks, stride, num_classes=6):\n    super(CNN, self).__init__()\n\n    self.conv1= nn.Sequential(\n        nn.Conv2d(in_channels= 1, out_channels=blocks[0], kernel_size=3, stride=2, padding=3),\n        nn.BatchNorm2d(64)\n    )\n    num_blocks= len(blocks)\n    self.blocks= nn.ModuleList([\n         Block(in_channels= blocks[i], out_channels= blocks[i]*2, stride= stride[i]) for i in range(num_blocks)\n    ])\n\n    self.avgpool= nn.AdaptiveAvgPool2d((1,1))\n    self.fc= nn.Linear(blocks[-1]*2, num_classes)\n\n  def forward(self,x):\n    x= self.conv1(x)\n    for i, b in enumerate(self.blocks):\n        x= self.blocks[i](x)\n    x= self.avgpool(x)\n\n    x= x.view(x.size(0),-1)\n    return self.fc(x)\n\n\n\n\ncnn=  CNN(blocks= [64,128,256,512], stride=[1,2,2,2], num_classes=6)\n\ncnn.to(device)\n\ncnn.load_state_dict(\n    torch.load(CNN_PATH, weights_only=True, map_location= torch.device(device)))\ncnn.eval() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:57:58.951084Z","iopub.execute_input":"2025-12-07T13:57:58.951927Z","iopub.status.idle":"2025-12-07T13:57:59.831933Z","shell.execute_reply.started":"2025-12-07T13:57:58.951889Z","shell.execute_reply":"2025-12-07T13:57:59.830664Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"CNN(\n  (conv1): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (blocks): ModuleList(\n    (0): Block(\n      (conv1): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (resize): Sequential()\n    )\n    (1): Block(\n      (conv1): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (resize): Sequential()\n      (resizer): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): Block(\n      (conv1): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (resize): Sequential()\n      (resizer): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): Block(\n      (conv1): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (resize): Sequential()\n      (resizer): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=1024, out_features=6, bias=True)\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## ResCNN architecture","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n  \"\"\"\n  Description: Residual Block with skip connection\n\n  Input: \n  in_channels: int\n  Number of channels present in input\n  out_channels: int\n  Number of output channels\n  stride: 1\n  Step size a filter uses to move across an image\n  \n  Output: Object\n  Initialized instance of Residual block\n  \n  \"\"\"\n  def __init__(self, in_channels, out_channels,  stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1=nn.Sequential(\n        nn.Conv2d(in_channels= in_channels,\n                  out_channels= out_channels,\n                  kernel_size=3,\n                  stride=stride,\n                  padding=1  ),\n            nn.BatchNorm2d(num_features= out_channels ),\n            nn.ReLU()\n        )\n\n        self.conv2= nn.Sequential(\n            nn.Conv2d(in_channels= out_channels,\n                      out_channels= out_channels,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1  ),\n            nn.BatchNorm2d(num_features=out_channels )\n        )\n\n        self.resize=nn.Sequential()\n\n        if in_channels != out_channels or stride!=1:\n\n          self.resizer= nn.Sequential(\n              nn.Conv2d(in_channels= in_channels,\n                                out_channels= out_channels,\n                                kernel_size=1,\n                                stride=stride,\n                                padding=0\n                ),\n              nn.BatchNorm2d(num_features=out_channels )\n          )\n    \n  def forward(self, x): \n        identity = x \n \n        out = self.conv1(x)\n        out = self.conv2(out)\n \n        if self.resizer is not None:\n            identity = self.resizer(identity)\n         \n        out += identity \n \n        out = F.relu(out)\n        \n        return out\n\n\nclass ResCNN(nn.Module):\n  \"\"\"\n  Description: CNN with Residual blocks\n\n  Input:  \n  num_classes: int\n  Number of classes involved in classification task\n  stride: Array\n  Step size a filter uses for corresponding block to move across an image\n  \n  Output: Object\n  Initialized instance of CNN with Residual blocks\n  \n  \"\"\"\n  def __init__(self,blocks, stride, num_classes=6):\n    super(ResCNN, self).__init__()\n\n    self.conv1= nn.Sequential(\n        nn.Conv2d(in_channels= 1, out_channels=blocks[0], kernel_size=3, stride=2, padding=3),\n        nn.BatchNorm2d(64)\n    )\n    num_blocks= len(blocks)\n    self.blocks= nn.ModuleList([\n         ResidualBlock(in_channels= blocks[i], out_channels= blocks[i]*2, stride= stride[i]) for i in range(num_blocks)\n    ])\n\n    self.avgpool= nn.AdaptiveAvgPool2d((1,1))\n    self.fc= nn.Linear(blocks[-1]*2, num_classes)\n\n  def forward(self,x):\n    x= self.conv1(x)\n    for i, b in enumerate(self.blocks):\n        x= self.blocks[i](x)\n    x= self.avgpool(x)\n\n    x= x.view(x.size(0),-1)\n    return self.fc(x)\n\nres_cnn=  ResCNN(blocks= [64,128,256,512], stride=[1,2,2,2], num_classes=6)\n\nres_cnn.to(device)\n\n\nres_cnn.load_state_dict(\n    torch.load(RESCNN_PATH, weights_only=True, map_location= torch.device(device)))\nres_cnn.eval()\n\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:57:59.833293Z","iopub.execute_input":"2025-12-07T13:57:59.833608Z","iopub.status.idle":"2025-12-07T13:58:00.623570Z","shell.execute_reply.started":"2025-12-07T13:57:59.833576Z","shell.execute_reply":"2025-12-07T13:58:00.622314Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"ResCNN(\n  (conv1): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (blocks): ModuleList(\n    (0): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (resize): Sequential()\n      (resizer): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (resize): Sequential()\n      (resizer): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (resize): Sequential()\n      (resizer): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): ResidualBlock(\n      (conv1): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n      (conv2): Sequential(\n        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (resize): Sequential()\n      (resizer): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=1024, out_features=6, bias=True)\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Ensemble architecture","metadata":{}},{"cell_type":"code","source":"class Ensemble(nn.Module):\n  \"\"\"\n  Description: Stack ensemble model with first layer consisting of CNN and VIT and meta layer consisting of Logistic Regression Model\n\n  Input: \n  num_classes: int\n  Number of classes present in classification task\n\n  Output: Object\n  Initialized instance of Ensemble class\n  \n  \"\"\"\n\n  def __init__(self,num_classes=6):\n    super(Ensemble, self).__init__()\n\n    self.model1= CNN(blocks= [64,128,256,512], stride=[1,2,2,2], num_classes=num_classes)\n    self.model1.to(device)\n      \n    self.model2= timm.create_model(\n        \"vit_tiny_patch16_224\",   # tiny as base\n        pretrained=True,\n        img_size=(78,78),\n        patch_size=6,\n        in_chans=1,               # 1 channel (grayscale)\n        num_classes=num_classes\n    )\n    self.model2.to(device)\n      \n    self.meta_layer= nn.Linear(num_classes*2,num_classes) \n    \n    \n  def forward(self,x):\n      output1= self.model1(x)\n      output2= self.model2(x)\n\n      combined_input= torch.cat((output1,output2), dim=1)\n      return self.meta_layer(combined_input)\n\n\nensemble = Ensemble()\nensemble.to(device)\nensemble.load_state_dict(\n    torch.load(ENSEMBLE_PATH, weights_only=True, map_location= torch.device(device)))\nensemble.eval() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VIT Pretrained","metadata":{}},{"cell_type":"code","source":"\nvit = timm.create_model(\n    \"vit_tiny_patch16_224\",   # tiny as base\n    pretrained=True,\n    img_size=(78,78),\n    patch_size=6,\n    in_chans=1,               # 1 channel (grayscale)\n    num_classes=6\n)\n\nvit.to(device)\nvit.load_state_dict(\n    torch.load(VIT_PATH, weights_only=True, map_location= torch.device(device)))\nvit.eval() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:45:27.284449Z","iopub.execute_input":"2025-12-07T14:45:27.284842Z","iopub.status.idle":"2025-12-07T14:45:27.862333Z","shell.execute_reply.started":"2025-12-07T14:45:27.284819Z","shell.execute_reply":"2025-12-07T14:45:27.861242Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(1, 192, kernel_size=(6, 6), stride=(6, 6))\n    (norm): Identity()\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (patch_drop): Identity()\n  (norm_pre): Identity()\n  (blocks): Sequential(\n    (0): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (1): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (2): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (3): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (4): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (5): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (6): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (7): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (8): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (9): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (10): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (11): Block(\n      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=192, out_features=576, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=192, out_features=768, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=768, out_features=192, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n  (fc_norm): Identity()\n  (head_drop): Dropout(p=0.0, inplace=False)\n  (head): Linear(in_features=192, out_features=6, bias=True)\n)"},"metadata":{}}],"execution_count":48},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"Genius-Society/HEp2\", split=\"train\")\ntrain_split= ds.train_test_split(test_size=0.3, seed=42)  \n\ntest_set= train_split['test'].train_test_split(test_size=0.5, seed=42) \n \ntest_set= test_set['train']\n\n# store label names for classification report\nlabel_names=ds.features[\"label\"].names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:58:04.340687Z","iopub.execute_input":"2025-12-07T13:58:04.340998Z","iopub.status.idle":"2025-12-07T13:58:12.501985Z","shell.execute_reply.started":"2025-12-07T13:58:04.340975Z","shell.execute_reply":"2025-12-07T13:58:12.501033Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59075a44a4504086a1257eab69c04556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.zip:   0%|          | 0.00/54.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7716e72b71394eb1a784324e7af9ca2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/13596 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1df170dab46c43eea14defc1892fe4f0"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Testing functions","metadata":{}},{"cell_type":"code","source":"def check_by_label(pred,real, index=[0]):\n    \"\"\"\n        Description: \n        Display label names of pred and real label_ids\n        -----------------------------------------------------------------------------------\n        Input:\n        pred: Array\n        Predicted labels of test data\n        real: Array\n        Actual labels of test data\n        -----------------------------------------------------------------------------------\n        Output: None \n        \n    \"\"\"\n    pred_label_names= [label_names[p] for p in pred]\n    test_label_names= [label_names[p] for p in real]\n    for i in index:\n        print(f\"Pattern '{test_label_names[i]}' got classified as Pattern '{pred_label_names[i]}'\")\n\n\ndef classification_matrix(pred,real, label_names= label_names):\n    \"\"\"\n        Description: \n        Display classification matrix\n        -----------------------------------------------------------------------------------\n        Input:\n        pred: Array\n        Predicted labels of test data\n        real: Array\n        Actual labels of test data\n        -----------------------------------------------------------------------------------\n        Output: None \n        \n    \"\"\"\n    \n    print(classification_report(real, pred, digits=2,labels= [0,1,2,3,4,5], target_names = label_names,zero_division=1.0))\n\n\ndef test_dataset(model,test_data):\n    \"\"\"\n        Description: \n        Performing testing on model using inputted dataset\n        -----------------------------------------------------------------------------------\n        Input:\n        model: Object\n        model used for prediction\n        test_data: DataLoader\n        Test data\n        -----------------------------------------------------------------------------------\n        Output: \n        test_preds: Array\n        Predicted labels for test images\n        test_labels: Array\n        Actual labels of test images\n        \n    \"\"\"\n    test_pred= []\n    test_labels= []\n    \n    with torch.no_grad():\n        for i, batch in enumerate(test_data):\n            image = batch['image'].to(device, non_blocking=True)\n            labels= batch['label']\n            outputs= model(image)\n            _, preds= torch.max(outputs, dim=1)\n    \n            test_pred.extend(preds.cpu().numpy())\n            test_labels.extend(labels.cpu().numpy())\n    \n        test_preds= np.array(test_pred)\n        test_labels= np.array(test_labels)\n    \n    return test_preds, test_labels\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:55:33.293047Z","iopub.execute_input":"2025-12-07T14:55:33.293422Z","iopub.status.idle":"2025-12-07T14:55:33.303766Z","shell.execute_reply.started":"2025-12-07T14:55:33.293393Z","shell.execute_reply":"2025-12-07T14:55:33.302459Z"}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"class Normalize:\n    \"\"\"\n    Description: Preprocessing function to perform image enhancement via image stretching using \n    the 1st and 99th quantile intensities and normalization.\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def __call__(self,img):  \n        \"\"\"\n        Input: torch.Tensor\n        Image to be normalized\n        \n        Output: torch.Tensor\n        Normalized image\n        \"\"\"\n        img_flatten = img.flatten()\n        q1= torch.quantile(img_flatten, 0.01)\n        q99 = torch.quantile(img_flatten, 0.99)\n\n        if q1== q99:\n            return torch.clamp(img, 0,1)\n\n        stretch_img = torch.clamp(img, q1,q99) \n        normalized = ((stretch_img - q1) / (q99  - q1)) #*(255-0) + 0\n\n        return torch.clamp(normalized, min= 0, max= 1.0) \n\n\n\ndef apply_preproc(data ):\n    \"\"\"\n        Description:\n        Initialize Transform function to apply basic preprocessing by conversion of data to tensor\n        and performing image normalization\n        -----------------------------------------------------------------------------------\n        Input:\n            data: \n            Input a row from the dataset consisting of images and corresponding labels \n        -----------------------------------------------------------------------------------\n        Output: dict\n        Return transformed row\n        \n    \"\"\"\n    transform= tr.Compose([ \n                              tr.ToTensor(), \n                              Normalize(), \n                        ])\n    return {'image': transform(data['image']), 'label': data['label']}  \n\ndef view_image(image):\n    view_img= tr.Compose([ \n        tr.ToTensor(),\n                    tr.RandomRotation(degrees=(18)) , \n                    tr.ColorJitter(brightness=np.random.uniform(0.1,0.15)),\n                    # tr.ElasticTransform(alpha=np.random.uniform(49.7, 50), sigma=5.0),\n                           tr.ToPILImage()\n                ]) \n    return view_img(image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:20:31.852280Z","iopub.execute_input":"2025-12-07T14:20:31.852701Z","iopub.status.idle":"2025-12-07T14:20:31.862815Z","shell.execute_reply.started":"2025-12-07T14:20:31.852675Z","shell.execute_reply":"2025-12-07T14:20:31.861701Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Initialize dataset","metadata":{}},{"cell_type":"code","source":"\nclass Hep2Dataset:\n    \"\"\"\n    Custom dataset to prepare images to be loaded into the dataloader.\n    \"\"\"\n    def __init__(self, data, set_type=\"\", rotation=18):\n        \"\"\"\n        Description:\n        Initialize Dataset as an object\n        -----------------------------------------------------------------------------------\n        Input:\n            data: \n            Input dataset\n            \n            set_type: str, \"\" default\n            Determine input dataset is for training purpose.\n            If set_type = \"augment\", dataset is meant for data augmentation.\n            \n            rotation: int, 18 default\n            Degree of rotation for image augmentation.\n        -----------------------------------------------------------------------------------\n        Output: None\n        \n        \"\"\"\n        super().__init__() \n\n        self.ds= data\n        self.set_type= set_type\n\n        self.labels= data.to_pandas()['label'] #labels of each image in the dataset\n        \n        \n        self.transform=None\n        if set_type== \"augment\":\n            # Initialize transform for dataaugmentation using random rotation and brightness adjustment\n            self.transform=tr.Compose([ \n                    tr.RandomRotation(degrees=(rotation)) ,\n                    tr.ColorJitter(brightness=0.1)\n                ]) \n\n    def get_weights(self):\n        \"\"\"\n        Description: \n        Generate weights for the dataset for random sampling\n        -----------------------------------------------------------------------------------\n        Input: None\n        -----------------------------------------------------------------------------------\n        Output:\n        sample_weights: array\n        List of weights for each label\n        \"\"\"\n        label_counts= self.labels.value_counts() \n        sample_weights= self.labels.map(1/label_counts).tolist()\n        return sample_weights\n        \n    def __len__(self):\n        \"\"\"\n        Description: \n        Returns length of dataset\n        -----------------------------------------------------------------------------------\n        Input:None\n        -----------------------------------------------------------------------------------\n        Output: int\n        length of dataset \n        \"\"\"\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Description: \n        Returns image and corresponding label at a particular index\n        -----------------------------------------------------------------------------------\n        Input:\n        idx: int\n        Index to retrieve image and label from\n        -----------------------------------------------------------------------------------\n        Output: dict\n        Dictionary with image and label at a particular index\n        \n        \"\"\"\n        if self.set_type == \"augment\":\n            image= self.transform(self.ds[idx]['image'])\n        else:\n            image= self.ds[idx]['image']\n        \n        label = self.ds[idx]['label']\n        \n        return {'image': image, 'label' : label}\n\n\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:58:12.531615Z","iopub.execute_input":"2025-12-07T13:58:12.531901Z","iopub.status.idle":"2025-12-07T13:58:12.550033Z","shell.execute_reply.started":"2025-12-07T13:58:12.531879Z","shell.execute_reply":"2025-12-07T13:58:12.548390Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"num_proc= 4 \nsamples= 5 #number of samples to be included from each label type\ndef load_subsets(data= test_set, num_proc=num_proc):\n    \"\"\"\n        Description: \n        Create sub-datasets which consist of only one particular label with index corresponding to label_id\n        -----------------------------------------------------------------------------------\n        Input:\n        data: datasets.arrow_dataset.Dataset\n        Data to be divided into sub-datasets based on their label ids\n        -----------------------------------------------------------------------------------\n        Output: Array\n        Array of datasets containing data corresponding to one particular label id\n        \n    \"\"\"\n    \n    label_samples=[]\n    for i in range(6):\n        print(f\"filtering for label {i}...\")\n        label_samples.append( \n            data.filter(lambda x:  x['label'] == i,num_proc=num_proc).shuffle()\n        )\n    return label_samples\n\n\nlabel_samples= load_subsets()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T13:58:12.552383Z","iopub.execute_input":"2025-12-07T13:58:12.552685Z","iopub.status.idle":"2025-12-07T14:13:22.847856Z","shell.execute_reply.started":"2025-12-07T13:58:12.552656Z","shell.execute_reply":"2025-12-07T14:13:22.846534Z"}},"outputs":[{"name":"stdout","text":"filtering for label 0...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=4):   0%|          | 0/2039 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b79974ca364764a24efb02830b50b9"}},"metadata":{}},{"name":"stdout","text":"filtering for label 1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=4):   0%|          | 0/2039 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a6eb236af7a44b099e5027cbb3f9c22"}},"metadata":{}},{"name":"stdout","text":"filtering for label 2...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=4):   0%|          | 0/2039 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24c1e1aeaf604fd0a322abe12749a465"}},"metadata":{}},{"name":"stdout","text":"filtering for label 3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=4):   0%|          | 0/2039 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68bda012447c4786b93e3898a73dbf61"}},"metadata":{}},{"name":"stdout","text":"filtering for label 4...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=4):   0%|          | 0/2039 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f75610036fb54484b1343bc48b32a8c7"}},"metadata":{}},{"name":"stdout","text":"filtering for label 5...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=4):   0%|          | 0/2039 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3230bc40b49d45c9bbbee0f26e50e6e2"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"\nBATCH_SIZE=64\nNUM_WORKERS=4\n  \ndef defined_inputs(data):\n        \"\"\"\n        Description: \n        Preprocess the dataset and load it into data loader\n        -----------------------------------------------------------------------------------\n        Input:\n        data: datasets.arrow_dataset.Dataset or dict\n        Dataset to be preprocessed\n        -----------------------------------------------------------------------------------\n        Output: datasets.Dataset\n        Dataloader of inputted dataset\n        \n        \"\"\"\n        if isinstance(data, dict): \n                print(\"converting to hugging face format\")\n                data= Dataset.from_dict(data)\n            \n        # if isinstance(data, datasets.arrow_dataset.Dataset):\n        print(\"Applying initial preprocessing..\")\n        data = data.map(\n                apply_preproc,  \n                batched=False\n            )\n        data= data.with_format('torch') \n\n        print(\"loading data\", type(data)) \n        dataset= Hep2Dataset(data)\n        \n        data_loader= DataLoader(dataset,\n                                batch_size= BATCH_SIZE,\n                                prefetch_factor=2,\n                                num_workers=NUM_WORKERS,\n                                pin_memory=True,\n                                persistent_workers=True ,\n                                shuffle=True)\n        return data_loader\n            \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:57:48.573250Z","iopub.execute_input":"2025-12-07T14:57:48.573705Z","iopub.status.idle":"2025-12-07T14:57:48.582553Z","shell.execute_reply.started":"2025-12-07T14:57:48.573679Z","shell.execute_reply":"2025-12-07T14:57:48.581243Z"}},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"# Test on data","metadata":{}},{"cell_type":"code","source":"def classify(model=cnn, test_data=test_data):\n    \"\"\"\n        Description: \n        Test on inputted models using inputed test data and display classification matrix\n        -----------------------------------------------------------------------------------\n        Input:\n        models: Array\n        Array of models to train upon\n        \n        test_data: DataLoader\n        Data to be tested upon\n        -----------------------------------------------------------------------------------\n        Output: dict\n        Dictionary containing actual and predicted labels for each model\n        \n        \"\"\" \n    \n    print(f\"Testing on model {model.__class__.__name__}..\")\n        \n    preds, labels= test_dataset(model,test_data) \n     \n    return {\"preds\": preds, \"real\": labels}\n\n\n# classify()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:31:10.146556Z","iopub.execute_input":"2025-12-07T14:31:10.147008Z","iopub.status.idle":"2025-12-07T14:31:10.153920Z","shell.execute_reply.started":"2025-12-07T14:31:10.146978Z","shell.execute_reply":"2025-12-07T14:31:10.152583Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## Pre-defined inputs","metadata":{}},{"cell_type":"code","source":"\ndef predefine_data(label_samples=label_samples, samples=5):\n    # Create pre-defined dataset using given number of samples\n    \"\"\"\n        Description: \n        Create pre-defined dataset using given number of samples from each label\n        -----------------------------------------------------------------------------------\n        Input:\n        data: Array\n        Array of datasets from which data is to be sampled from\n        -----------------------------------------------------------------------------------\n        Output: datasets.arrow_dataset.Dataset\n        Combined dataset containing 5 samples of each label\n        \n    \"\"\"\n    print(f\"Fetching {samples} samples from each class\")\n    sample_data= [ Dataset.from_dict(label_samples[i][:samples]) for i in range(6)]\n    predefined_samples= datasets.concatenate_datasets(sample_data)\n    print(f\"Created dataset of {len(predefined_samples)} samples\")\n    return predefined_samples\n\npredefined_test = predefine_data()\n\npredef_data=defined_inputs(predefined_test) \nmodels= [vit,cnn,res_cnn, ensemble]\n\nfor model in models:\n    outputs= classify(model=model, test_data=predef_data) \n    classification_matrix(outputs[\"preds\"], outputs[\"real\"]) \n    break ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T15:02:22.426129Z","iopub.execute_input":"2025-12-07T15:02:22.426766Z","iopub.status.idle":"2025-12-07T15:02:29.884331Z","shell.execute_reply.started":"2025-12-07T15:02:22.426691Z","shell.execute_reply":"2025-12-07T15:02:29.882407Z"}},"outputs":[{"name":"stdout","text":"Fetching 5 samples from each class\nCreated dataset of 30 samples\nApplying initial preprocessing..\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44af4b6a59e642eb96f0a8e0d35699a9"}},"metadata":{}},{"name":"stdout","text":"loading data <class 'datasets.arrow_dataset.Dataset'>\nTesting on model VisionTransformer..\n              precision    recall  f1-score   support\n\n  Centromere       0.83      1.00      0.91         5\n       Golgi       1.00      0.60      0.75         5\n Homogeneous       1.00      1.00      1.00         5\n       NuMem       1.00      1.00      1.00         5\n   Nucleolar       0.83      1.00      0.91         5\n    Speckled       0.80      0.80      0.80         5\n\n    accuracy                           0.90        30\n   macro avg       0.91      0.90      0.89        30\nweighted avg       0.91      0.90      0.89        30\n\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"## User defined inputs","metadata":{}},{"cell_type":"code","source":"# Taking 2 samples\nchosen_sample = label_samples[0][0] \nchosen_sample2 = label_samples[4][0]\n \nuser_defined = {\n    \"image\": [chosen_sample['image'],chosen_sample2['image']],\n    \"label\": [chosen_sample['label'],chosen_sample2['label']]\n}\ntest_data= defined_inputs(user_defined)\n\n# Preview an image\nview_image(chosen_sample['image']) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T15:01:37.717266Z","iopub.execute_input":"2025-12-07T15:01:37.718574Z","iopub.status.idle":"2025-12-07T15:01:38.082499Z","shell.execute_reply.started":"2025-12-07T15:01:37.718514Z","shell.execute_reply":"2025-12-07T15:01:38.081352Z"}},"outputs":[{"name":"stdout","text":"converting to hugging face format\nApplying initial preprocessing..\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f69a8a1e1b6940558fd79444a15582e7"}},"metadata":{}},{"name":"stdout","text":"loading data <class 'datasets.arrow_dataset.Dataset'>\n","output_type":"stream"},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"<PIL.Image.Image image mode=L size=78x78>","image/png":"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","image/jpeg":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABOAE4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+pBwBSj3pwUlgAM5qZ7d4gpdcBumaiwDR5SsemPeojER0OaaVZeoNJSgZIqQDNOVcmp1iZCGxg9qsPJPfyBDtBQE46YA5qmQR70o+lABNamn2pcJIzKqs+z5ux/ya6S48M6ejiOe3iLoAN0TFdwPPY4PXrXnyjvTxV2yjhcu00oTauVGCSTVu4uWu3Duiq20LhRjpRb2LzTbAp3EgY+vapBod41+tl5DC4bojDBOfrVK4tnt53iI+dDgj3qNoWjGWBGelT2M6RTo0m4oGyQpwa7XU9cg1+5hlhtvsnlQKjMh/1hHAJ98YHvjPHSvNhwMU9RW7pOmwTWU91JcKHiwFhI5bPekltS4Z0UjHWtDw1qF1a6zBexK7PbMJSyDJUL3+lbN2dQvdck1OaFpXu2JRlP3XfJGD2IJrGh0+Ga7mW6uBHJEGaQt/e9B681k38zXEgZlC7RtGOOKoJ96u88JWugJufxJNcwwPHmFrcj5jnnOQfevOh1qSNSzAetdv4NvbDQdWdtYsjPBtKsoPQnpn/Paqms6lHHeSxWCKIN7GNwCCQapaUtzDIZ4yQuCrEdwRgj8s1148TaRE8sSW0rQpCfs6bsbZSMfMepA5H5c1ydtH5t35kgygbLccGtLxkukTXP2zR0NvbuqjyDztIXn8z/WuQzhsirEMzOoRn4XoGPArOBqSNirAg4PrXWSNcWvhSJrm2BjupsxT5+bKjlT7YIxn8Koy3UmrXEQYRxLEgRQoACgep6mtHTr+FYvssrEKO3qah0nTm1HWoLdY/MMkuAmSd2Oo4rp9P1D+xL7Vp/slssbwtCIZxwvIA465GPzrgb2fzXIHQnPFUpY3hYBh1GRUYfb/ABYqGnA4NTmd2QIzEgHgE9KmgD7lIHXpV+WYi7lNzbgNtwF5G2pNE1CWz1KOaK8ltHUNiWPO4ZBGB9c4rVh0y+1aK9uow86wq0krsxJA9T61zV60IlzATtx365qozlup5qCQ84opR0pQa17bWXt7A2yRRZ8wOkhHzIR6H34/Kq630itIxwzSAgkjnmq4c5yOtaK67ex6bLYLKRbysGdOxIrLdiSaYTUROTml3Uuc0opd1ODcVPbmPnf6VA7fMducUmeKaTgVHRRS5NKDk04+1ANHalzTGPAptf/Z"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"\noutput= classify(model=vit,test_data=test_data)\nprint(output)\n# To view label names of classification output\ncheck_by_label(output[\"preds\"], output[\"real\"], index= [0,1])\n\n# To build classification matrix only when there is data for all 6 labels present\nclassification_matrix(output[\"preds\"], output[\"real\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T15:01:40.485504Z","iopub.execute_input":"2025-12-07T15:01:40.485899Z","iopub.status.idle":"2025-12-07T15:01:40.718508Z","shell.execute_reply.started":"2025-12-07T15:01:40.485873Z","shell.execute_reply":"2025-12-07T15:01:40.716913Z"}},"outputs":[{"name":"stdout","text":"Testing on model VisionTransformer..\n{'preds': array([4, 0]), 'real': array([4, 0])}\nPattern 'Nucleolar' got classified as Pattern 'Nucleolar'\nPattern 'Centromere' got classified as Pattern 'Centromere'\n              precision    recall  f1-score   support\n\n  Centromere       1.00      1.00      1.00         1\n       Golgi       1.00      1.00      1.00         0\n Homogeneous       1.00      1.00      1.00         0\n       NuMem       1.00      1.00      1.00         0\n   Nucleolar       1.00      1.00      1.00         1\n    Speckled       1.00      1.00      1.00         0\n\n   micro avg       1.00      1.00      1.00         2\n   macro avg       1.00      1.00      1.00         2\nweighted avg       1.00      1.00      1.00         2\n\n","output_type":"stream"}],"execution_count":66}]}